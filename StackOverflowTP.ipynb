{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lxml.html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse xml data\n",
    "\n",
    "def get_questions_tags(root):\n",
    "    questions = []\n",
    "    tags = []\n",
    "    code = []\n",
    "    titles = []\n",
    "    users = []\n",
    "    ques_with_title = []\n",
    "    for row in root.findall('row'):\n",
    "        post = row.get(\"PostTypeId\")\n",
    "        user = row.get(\"OwnerUserId\")\n",
    "        post_type = BeautifulSoup(post, \"lxml\")\n",
    "        if post_type.get_text() == \"1\" and user is not None:\n",
    "            \n",
    "            users.append(user)\n",
    "            \n",
    "            #Get the Questions\n",
    "            body = row.get(\"Body\")\n",
    "            soup = BeautifulSoup(body, \"lxml\")\n",
    "            [s.extract() for s in soup('code')] \n",
    "            question_s = soup.get_text()\n",
    "#             q_set = nltk.word_tokenize(question_s)\n",
    "#             q_set = question_s.split()\n",
    "#             print(q_set)\n",
    "            question = nltk.Text(question_s)\n",
    "# #             print(question)\n",
    "            questions.append(question)\n",
    "            \n",
    "            #Get the Tags\n",
    "            tag_list = row.get(\"Tags\")\n",
    "            tag_str = re.sub('[<>]', ' ', tag_list)\n",
    "#             tag_set = nltk.word_tokenize(tag_str)\n",
    "            tag_set = tag_str.split()\n",
    "            tag_text = nltk.Text(tag_set)\n",
    "            tags.append(tag_set)\n",
    "\n",
    "            #Get the Titles\n",
    "            title_s = row.get(\"Title\")\n",
    "#             t_set = nltk.word_tokenize(title_s)\n",
    "#             t_set = title_s.split()\n",
    "#             print(title_s)\n",
    "            title = nltk.Text(title_s)\n",
    "            titles.append(title)\n",
    "            \n",
    "            #Get Title with First 400 and last 100 words from question body\n",
    "            \n",
    "            if (len(question_s) > 500):\n",
    "                question_s = question_s[:400] + question_s[-100:]\n",
    "            q_with_t = title_s + \" \" + question_s\n",
    "#             qt_set = nltk.word_tokenize(q_with_t)\n",
    "            qt_set = q_with_t.split()\n",
    "            ques_title = nltk.Text(qt_set)\n",
    "            ques_with_title.append(ques_title)\n",
    "\n",
    "#     print(len(ques_with_title))\n",
    "    return questions, tags, titles, ques_with_title, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove numbers and special characters from question\n",
    "\n",
    "def number_removal(ques_list):\n",
    "    q_with_t_list = []\n",
    "    for ques in ques_list:\n",
    "        q_t_text = \"\"\n",
    "        for word in ques:\n",
    "            characters = [\".\", \",\", \":\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"?\", \"'\"]\n",
    "            q_text = ''.join([i for i in word if not (i.isdigit() or [e for e in characters if e in i])])\n",
    "            if q_text != '':\n",
    "                q_t_text += q_text + \" \"\n",
    "        qt_set = q_t_text.split()\n",
    "#         qt_set = nltk.word_tokenize(q_t_text)\n",
    "        title_ques = nltk.Text(qt_set)\n",
    "        q_with_t_list.append(title_ques)\n",
    "    return q_with_t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to lowercase and remove stopwords from the question body\n",
    "\n",
    "def remove_stopwords(q_list):\n",
    "    q_with_t_list = []\n",
    "    for text in q_list:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        st = \"\"\n",
    "        for w in text:\n",
    "            if w.lower() not in stopwords:\n",
    "                st += w.lower() + \" \"\n",
    "        w_set = st.split()\n",
    "#         w_set = nltk.word_tokenize(st)\n",
    "#         ques_body = nltk.Text(st)\n",
    "        q_with_t_list.append(st)\n",
    "    return q_with_t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform stemming and converting each word in the question to its root word\n",
    "\n",
    "def stemming(q_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    post = []\n",
    "    for q in q_list:\n",
    "        st = \"\"\n",
    "        for word in q:\n",
    "            st += stemmer.stem(word) + \" \"\n",
    "#         w_set = nltk.word_tokenize(st)\n",
    "        w_set = st.split()\n",
    "#         ques_body = nltk.Text(w_set)\n",
    "        post.append(w_set)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime0 = datetime.now()\n",
    "s = preprocessor('Posts_small.xml')\n",
    "print(\"Time elapsed in Pre-Processing: \" + str(datetime.now() - startTime0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor component - Tokenisation, Number removal, Stop-word removal, Stemming\n",
    "\n",
    "def preprocessor(filename):\n",
    "    root = ET.parse(filename).getroot()\n",
    "    questions, tags, titles, ques_with_title, users = get_questions_tags(root)\n",
    "    ques_with_title_list = number_removal(ques_with_title)\n",
    "#     print(ques_with_title_list)\n",
    "    title_ques = remove_stopwords(ques_with_title_list)\n",
    "#     print(title_ques)\n",
    "#     posts = stemming(title_ques)\n",
    "    frame = [titles, questions, ques_with_title, title_ques, tags, users]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return n most frequent tags from the training set\n",
    "\n",
    "startTime0 = datetime.now()\n",
    "def top_tags(rawTags, num):\n",
    "    tagset = {}\n",
    "    sortedTags = []\n",
    "    for tags in rawTags: \n",
    "        for tag in tags:\n",
    "            if tag not in tagset:\n",
    "                tagset[tag] = 1\n",
    "            else:\n",
    "                tagset[tag] += 1\n",
    "    print(\"Total no. of unique tags in data: \", len(tagset))\n",
    "    sortedTags1 = sorted(tagset.items(), key=lambda item: item[1], reverse = True)\n",
    "    for tag in sortedTags1[0:num]:\n",
    "        sortedTags.append(tag[0])\n",
    "    \n",
    "    return sortedTags\n",
    "\n",
    "# Function to add Tag-vector to the Pre-Processed Data (Custom multiLabelBinarizer)\n",
    "\n",
    "def add_tag_mapping(preProcessedInput, mappingSize):\n",
    "    tagMap = []\n",
    "    rankedTags = top_tags(preProcessedInput[4], mappingSize)\n",
    "    \n",
    "    for tags in preProcessedInput[4]:\n",
    "        tagVector = []\n",
    "#         tagVector = [0]*mappingSize\n",
    "        for tag in tags:\n",
    "            if tag in rankedTags:\n",
    "                tagVector.append(tag)\n",
    "\n",
    "        tagMap.append(tagVector)\n",
    "    preProcessedInput.append(tagMap)\n",
    "    return preProcessedInput\n",
    "\n",
    "\n",
    "processedData = add_tag_mapping(preprocessor('train2.xml'), 20)\n",
    "df = pd.DataFrame({'titles' : processedData[0],\n",
    "                      'questions' : processedData[1],\n",
    "                      'ques_with_title' : processedData[2],\n",
    "                      'posts' : processedData[3],\n",
    "                      'tags' : processedData[4],\n",
    "                      'users' : processedData[5],\n",
    "                     'tag_vector' : processedData[6] })\n",
    "\n",
    "\n",
    "print(\"Time elapsed in PreProcessing: \" + str(datetime.now() - startTime0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Large Sample run\n",
    "\n",
    "startTime01 = datetime.now()\n",
    "processedData = add_tag_mapping(preprocessor('sample4.xml'), 20)\n",
    "df = pd.DataFrame({'titles' : processedData[0],\n",
    "                      'questions' : processedData[1],\n",
    "                      'ques_with_title' : processedData[2],\n",
    "                      'posts' : processedData[3],\n",
    "                      'tags' : processedData[4],\n",
    "                      'users' : processedData[5],\n",
    "                     'tag_vector' : processedData[6] })\n",
    "\n",
    "print(\"Time elapsed in PreProcessing: \" + str(datetime.now() - startTime01))\n",
    "\n",
    "startTime0 = datetime.now()\n",
    "npx=np.array(df[\"posts\"])\n",
    "npy=np.array(df[\"tag_vector\"])\n",
    "# tv = TfidfVectorizer() # can play around with min_df/max_df here\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(npx, npy, test_size=0.2)\n",
    "\n",
    "# x_traincv = cv.fit_transform([\"Hi How are you How are you doing\",\"Hi what's up\",\"Hey there\"])\n",
    "# x_traincv.toarray()\n",
    "# cv.get_feature_names()\n",
    "\n",
    "# x_traintv=tv.fit_transform(x_train)\n",
    "\n",
    "# a=x_traintv.toarray()\n",
    "# print(tv.inverse_transform(a[0]))\n",
    "# # print(x_train.iloc[0])\n",
    "# print(tv.get_feature_names())\n",
    "\n",
    "# x_testtv=tv.transform(x_test)\n",
    "\n",
    "print(\"Training Data size: \", len(x_train))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train = mlb.fit_transform(y_train)\n",
    "# print(\"y_train is: \", Y_train)\n",
    "\n",
    "\n",
    "startTime1 = datetime.now()\n",
    "classifier1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(loss='hinge', tol = .001)))])\n",
    "\n",
    "classifier1.fit(x_train, Y_train)\n",
    "predicted1 = classifier1.predict(x_test)\n",
    "print(\"Time elapsed in LinearSVC: \", str(datetime.now() - startTime1))\n",
    "\n",
    "\n",
    "# startTime2 = datetime.now()\n",
    "# classifier2 = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', OneVsRestClassifier(svm.SVC(kernel='linear', C=1, gamma=1)))])\n",
    "\n",
    "# classifier2.fit(x_train, Y_train)\n",
    "# predicted2 = classifier2.predict(x_test)\n",
    "# print(\"Time elapsed in Linear svm: \", str(datetime.now() - startTime2))\n",
    "\n",
    "\n",
    "startTime3 = datetime.now()\n",
    "classifier3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', max_iter=200)))])\n",
    "\n",
    "classifier3.fit(x_train, Y_train)\n",
    "predicted3 = classifier3.predict(x_test)\n",
    "print(\"Time elapsed in SGDClassifier: \", str(datetime.now() - startTime3))\n",
    "\n",
    "\n",
    "# all_labels = mlb.inverse_transform(predicted)\n",
    "# for item, labels in zip(x_test, all_labels):\n",
    "#     print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "\n",
    "# model = svm.SVC(kernel='linear', C=1, gamma=1)\n",
    "# print(y_train.to_string(index=False))\n",
    "# mnb = MultinomialNB()\n",
    "# mnb.fit(x_traincv,y_train)\n",
    "\n",
    "print(\"\\nAccuracy for LinearSVC: \", accuracy_score(predicted1, mlb.fit_transform(y_test)))\n",
    "print(\"F1 for LinearSVC: \", f1_score(predicted1, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "# print(\"\\nAccuracy for Linear SVM: \", accuracy_score(predicted2, mlb.fit_transform(y_test)))\n",
    "# print(\"F1 for Linear SVM: \", f1_score(predicted2, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "print(\"\\nAccuracy for SGD: \", accuracy_score(predicted3, mlb.fit_transform(y_test)))\n",
    "print(\"F1 for SGD: \", f1_score(predicted3, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "print(\"Time elapsed in this step: \" + str(datetime.now() - startTime0))\n",
    "# model.fit(x_traintv,Y_train)\n",
    "# clf = OneVsRestClassifier(svm.SVC(kernel='linear', C=1, gamma=1))\n",
    "# y_score = clf.fit(x_train, y_train).decision_function(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = mlb.inverse_transform(predicted1)\n",
    "for item, labels in zip(x_test, all_labels):\n",
    "    print('{0} => {1}'.format(item, ', '.join(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Smaller Sample run:\n",
    "\n",
    "startTime01 = datetime.now()\n",
    "processedData = add_tag_mapping(preprocessor('train2.xml'), 20)\n",
    "df = pd.DataFrame({'titles' : processedData[0],\n",
    "                      'questions' : processedData[1],\n",
    "                      'ques_with_title' : processedData[2],\n",
    "                      'posts' : processedData[3],\n",
    "                      'tags' : processedData[4],\n",
    "                      'users' : processedData[5],\n",
    "                     'tag_vector' : processedData[6] })\n",
    "\n",
    "print(\"Time elapsed in PreProcessing: \" + str(datetime.now() - startTime01))\n",
    "\n",
    "startTime0 = datetime.now()\n",
    "npx=np.array(df[\"posts\"])\n",
    "npy=np.array(df[\"tag_vector\"])\n",
    "# tv = TfidfVectorizer() # can play around with min_df/max_df here\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(npx, npy, test_size=0.2)\n",
    "\n",
    "# x_traincv = cv.fit_transform([\"Hi How are you How are you doing\",\"Hi what's up\",\"Hey there\"])\n",
    "# x_traincv.toarray()\n",
    "# cv.get_feature_names()\n",
    "\n",
    "# x_traintv=tv.fit_transform(x_train)\n",
    "\n",
    "# a=x_traintv.toarray()\n",
    "# print(tv.inverse_transform(a[0]))\n",
    "# # print(x_train.iloc[0])\n",
    "# print(tv.get_feature_names())\n",
    "\n",
    "# x_testtv=tv.transform(x_test)\n",
    "\n",
    "print(\"Training Data size: \", len(x_train))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train = mlb.fit_transform(y_train)\n",
    "# print(\"y_train is: \", Y_train)\n",
    "\n",
    "\n",
    "startTime1 = datetime.now()\n",
    "classifier1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(loss='hinge', tol = .001)))])\n",
    "\n",
    "classifier1.fit(x_train, Y_train)\n",
    "predicted1 = classifier1.predict(x_test)\n",
    "print(\"Time elapsed in LinearSVC: \", str(datetime.now() - startTime1))\n",
    "\n",
    "\n",
    "startTime2 = datetime.now()\n",
    "classifier2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(svm.SVC(kernel='linear')))])\n",
    "\n",
    "classifier2.fit(x_train, Y_train)\n",
    "predicted2 = classifier2.predict(x_test)\n",
    "print(\"Time elapsed in Linear svm: \", str(datetime.now() - startTime2))\n",
    "\n",
    "\n",
    "startTime3 = datetime.now()\n",
    "classifier3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', max_iter=200)))])\n",
    "\n",
    "classifier3.fit(x_train, Y_train)\n",
    "predicted3 = classifier3.predict(x_test)\n",
    "print(\"Time elapsed in SGDClassifier: \", str(datetime.now() - startTime3))\n",
    "\n",
    "\n",
    "# all_labels = mlb.inverse_transform(predicted)\n",
    "# for item, labels in zip(x_test, all_labels):\n",
    "#     print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "\n",
    "# model = svm.SVC(kernel='linear', C=1, gamma=1)\n",
    "# print(y_train.to_string(index=False))\n",
    "# mnb = MultinomialNB()\n",
    "# mnb.fit(x_traincv,y_train)\n",
    "\n",
    "print(\"\\nAccuracy for LinearSVC: \", accuracy_score(predicted1, mlb.fit_transform(y_test)))\n",
    "print(\"F1 for LinearSVC: \", f1_score(predicted1, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "print(\"\\nAccuracy for Linear SVM: \", accuracy_score(predicted2, mlb.fit_transform(y_test)))\n",
    "print(\"F1 for Linear SVM: \", f1_score(predicted2, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "print(\"\\nAccuracy for SGD: \", accuracy_score(predicted3, mlb.fit_transform(y_test)))\n",
    "print(\"F1 for SGD: \", f1_score(predicted3, mlb.fit_transform(y_test), average='micro'))\n",
    "\n",
    "print(\"Time elapsed in this step: \" + str(datetime.now() - startTime0))\n",
    "# model.fit(x_traintv,Y_train)\n",
    "# clf = OneVsRestClassifier(svm.SVC(kernel='linear', C=1, gamma=1))\n",
    "# y_score = clf.fit(x_train, y_train).decision_function(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = mlb.inverse_transform(predicted1)\n",
    "for item, labels in zip(x_test, all_labels):\n",
    "    print('{0} => {1}'.format(item, ', '.join(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of all possible tags\n",
    "\n",
    "def set_of_tags(tag_column):\n",
    "    tags = []\n",
    "    for row in tag_column:\n",
    "        for t in row:\n",
    "            if t not in tags:\n",
    "                tags.append(t)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
